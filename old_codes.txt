import express from "express";
import dotenv from 'dotenv';
import { RealtimeAgent, RealtimeSession } from '@openai/agents/realtime';
import { WebSocketServer } from "ws";
import http from "http"
import path from "path";


dotenv.config();

const app = express();
const server = http.createServer(app);
const wss = new WebSocketServer({ server });

app.use(express.static(path.join(process.cwd(), "client")));

//For WebSocket connection
wss.on("connection", async (ws) => {
    console.log("New WebSocket connection established");

    const agent = new RealtimeAgent({
        name: 'Assistant',
        instructions: `
        You are a multilingual voice assistant. You are friendly, helpful, and speak in a polite and concise tone.

        Supported languages:
        - English (en)
        - Spanish (es)
        - Turkish (tr)
        - French (fr)
        - German (de)
        - Italian (it)

        Instructions:
        - When the user speaks in one of the supported languages, always reply in that same language.
        - Never switch languages unless the user switches.
        - If the user speaks in a language you do not support, reply in English and say: "I'm sorry, I currently support only English, Spanish, Turkish, French, German, and Italian."
        - Do not attempt to translate, detect or guess unsupported languages.
        - Keep your responses natural, clear, and not overly formal.

        Important:
        - Do not mix languages in the same response.
        - Always maintain the conversation in the user's language, as long as it is supported.
        `
    });

    const session = new RealtimeSession(agent, {
        apiKey: process.env.OPENAI_API_KEY,
        transport: 'websocket',
        voice: "alloy",
        model: 'gpt-4o-realtime-preview-2025-06-03',
        config: {
            inputAudioFormat: 'pcm16',
            outputAudioFormat: 'pcm16',
            inputAudioTranscription: { model: 'whisper-1' },
            turnDetection: {
                type: 'server_vad',
                threshold: 0.5,
                prefix_padding_ms: 300,
                silence_duration_ms: 800
            }
        }
    });

    await session.connect({ apiKey: process.env.OPENAI_API_KEY });

    session.on('error', (err) => console.error('Session error:', err));


    let accumulatedAudio = Buffer.alloc(0);
    let isRecording = false;

    try {
        session.on("input_audio_buffer.speech_started", async () => {
            console.log("User speech started - interrupting AI");
            isRecording = true;
            try { await session.interrupt(); } catch (e) { console.log("interrupt err", e); }
        });

        session.on("input_audio_buffer.speech_stopped", ({delta}) => {
            ws.send(Buffer.from(delta.audio.buffer));
            console.log("User speech stopped, OpenAI is starting to create response.");
            isRecording = false;
        });


        // Listen to all transport events for debugging
        session.transport.on('*', async (event) => {
            console.log("Transport event:", event.type);
            if (event.type === 'error') {
                console.log("Transport error:", event.error);
            } else if (event.type === "audio_interrupted")
                try { await session.interrupt(); } catch (e) { console.log("interrupt err", e); }

            // Handle audio data directly from transport events
            if (event.type === 'response.audio.delta' && event.delta && ws.readyState === WebSocket.OPEN) {
                console.log('Transport: Sending audio delta to client');
                const audioBuffer = Buffer.from(event.delta, 'base64');
                ws.send(audioBuffer);
            }
        });

        ws.send(
            JSON.stringify({
                type: "session",
                data: {
                    model: "gpt-4o-realtime-preview",
                    session_id: session.id,
                    voice: session.voice,
                    model: session.model,
                    expires_at: Date.now() + 3600000 // 1 hour valid
                },
            })
        );



        // Handle messages from client
        ws.on("message", async (message, isBinary) => {
            try {
                if (isBinary) {
                    // Handle binary audio data
                    if (!isRecording) return;

                    const audioBuffer = Buffer.isBuffer(message) ? message : Buffer.from(message);
                    if (audioBuffer.length === 0) {
                        console.log("Received empty audio buffer, skipping");
                        return;
                    }

                    accumulatedAudio = Buffer.concat([accumulatedAudio, audioBuffer]);

                    // Send audio in chunks when we have enough data
                    if (accumulatedAudio.length >= 4800) { // 100ms worth of audio
                        try {
                            await session.sendAudio(accumulatedAudio, { commit: false });
                            console.log(`Sent ${accumulatedAudio.length} bytes to OpenAI`);
                            accumulatedAudio = Buffer.alloc(0); // Reset buffer
                        } catch (audioError) {
                            console.error("Error sending audio to OpenAI:", audioError);
                        }
                    }
                    return;
                }

                // Handle JSON messages
                const data = JSON.parse(message.toString());
                console.log("Received JSON message:", data.type);

                switch (data.type) {
                    case "start_conversation":
                        isRecording = true;
                        accumulatedAudio = Buffer.alloc(0);

                        try { await session.interrupt(); }
                        catch (e) { console.log("interrupt err", e); }

                        console.log("Started recording");
                        ws.send(JSON.stringify({
                            type: "conversation_started",
                            message: "Conversation Started",
                        }));
                        break;

                    case "stop_conversation":
                        isRecording = false;
                        console.log(`Stopping conversation. Buffer size: ${accumulatedAudio.length} bytes`);

                        // Send any remaining audio data and commit it
                        if (accumulatedAudio.length > 0) {
                            try {
                                await session.sendAudio(accumulatedAudio, { commit: true });
                                console.log(`Sent and committed final ${accumulatedAudio.length} bytes to OpenAI`);
                            } catch (audioError) {
                                console.error("Error sending/committing final audio to OpenAI:", audioError);
                            }
                        }


                        console.log("Server VAD should detect end of speech and generate response automatically");

                        accumulatedAudio = Buffer.alloc(0); // Reset buffer

                        ws.send(JSON.stringify({
                            type: "conversation_stopped",
                            message: "Conversation Stopped",
                        }));
                        break;

                    case "audio":
                        if (typeof data.audio === "string" && data.audio.length > 0) {
                            const audioBuffer = Buffer.from(data.audio, "base64");
                            if (audioBuffer.length > 0) {
                                await session.sendAudio(audioBuffer, { commit: false });
                            }
                        }
                        break;

                    case "mute_state":
                        isMuted = data.muted;
                        console.log(`Client mute state: ${isMuted}`);
                        break;

                    case "error":
                        console.error("Client error:", data.error);
                        break;

                    default:
                        console.log("Unknown message type:", data.type);
                }
            } catch (error) {
                console.error("Error processing message:", error);
                ws.send(JSON.stringify({
                    type: "error",
                    error: {
                        message: "The data type is not valid."
                    }
                }));
            }
        });

        // Handle connection close
        ws.on("close", () => {
            isRecording = false;
            session.close();
            console.log("WebSocket connection closed");
        });

    } catch (error) {
        console.error("Error creating session:", error);
        ws.send(JSON.stringify({
            type: "error",
            error: {
                message: "Failed to create session."
            }
        }));
    }
});

app.get("/", (req, res) => {
    res.sendFile("index.html", { root: "./client" });
});

server.listen(3000, () => {
    console.log("WebSocket Server listening on http://localhost:3000");
});


























const ws = new WebSocket("ws://localhost:3000");
ws.binaryType = "arraybuffer";

ws.onopen = () => {
    console.log("WS connected");
};

ws.onerror = (error) => {
    console.error("WebSocket error:", error);
};

ws.onclose = (event) => {
    console.log("WebSocket closed:", event.code, event.reason);
};

// Handle messages from server
ws.onmessage = async (e) => {
    if (typeof e.data === "string") {
        console.log("Received string message from server");
        const msg = JSON.parse(e.data);
        console.log("Parsed message:", msg);

        if (msg.type === "session") {
            document.getElementById("status").textContent = "BaÄŸlantÄ± hazÄ±r, konuÅŸabilirsiniz";
            console.log("Session ready");
        } else if (msg.type === "conversation_started") {
            console.log("Conversation started confirmation received");
        } else if (msg.type === "conversation_stopped") {
            console.log("Conversation stopped confirmation received");
        } else if (msg.type === "error") {
            console.error("Server error:", msg.error);
        }
    } else {
        console.log(`Received binary audio data: ${e.data.byteLength} bytes`);
        playAudio(e.data);
    }
};

let audioCtx, processor, source;

document.getElementById("voiceBtn").addEventListener("click", async () => {
    if (processor) {
        console.log("Stopping recording...");
        ws.send(JSON.stringify({ type: "stop_conversation" }));
        processor.disconnect();
        source.disconnect();
        await audioCtx.close();
        processor = null;
        document.getElementById("voiceBtn").textContent = "ðŸŽ¤";
        document.getElementById("status").textContent = "Stoped";
        viz.stop();
        return;
    } else {
        // Reset mute state on new recording
        isMuted = false;
        muteBtn.textContent = "ðŸ”‡";
        muteBtn.title = "Mute microphone";
    }

    console.log("Starting recording...");
    try {
        const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
                sampleRate: 48000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: true
            }
        });

        console.log("Got media stream");
        viz.start(stream);

        // AudioContext â€“ most browsers open at 48,000 Hz
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        console.log(`AudioContext sample rate: ${audioCtx.sampleRate}`);

        source = audioCtx.createMediaStreamSource(stream);

        // ScriptProcessor â€“ 16384 samples for better buffering
        processor = audioCtx.createScriptProcessor(16384, 1, 1);

        processor.onaudioprocess = (e) => {
            if (isMuted) {
                // Still process audio for visualization but don't send
                const floatBuf = e.inputBuffer.getChannelData(0);
                // ... (keep visualization code if needed)
                return;
            }



            const floatBuf = e.inputBuffer.getChannelData(0);

            // Check if we have actual audio data (not silence)
            let hasAudio = false;
            let maxAmplitude = 0;
            for (let i = 0; i < floatBuf.length; i++) {
                const amplitude = Math.abs(floatBuf[i]);
                maxAmplitude = Math.max(maxAmplitude, amplitude);
                if (amplitude > 0.001) {
                    hasAudio = true;
                }
            }

            // Downsample and PCM conversion
            const downsampled = downsample(floatBuf, audioCtx.sampleRate, 24000);
            if (!downsampled || downsampled.length === 0) {
                console.log("Downsampling failed or resulted in empty buffer");
                return;
            }

            const pcm16 = floatToPCM16(downsampled);

            if (!pcm16 || pcm16.length === 0) {
                console.log("PCM conversion failed or resulted in empty buffer");
                return;
            }

            // Send data in appropriate chunks - ensure minimum 100ms (2400 samples)
            if (pcm16.length >= 2400 && ws.readyState === WebSocket.OPEN) {
                if (hasAudio && maxAmplitude > 0.001) {
                    console.log(`Sending ${pcm16.length} samples (${pcm16.byteLength} bytes) to server, max amplitude: ${maxAmplitude.toFixed(4)}`);
                } else {
                    console.log(`Sending silence: ${pcm16.length} samples`);
                }
                ws.send(pcm16.buffer);
            } else if (pcm16.length > 0) {
                console.log(`Buffer too small: ${pcm16.length} samples, waiting for more data`);
            }
        };

        source.connect(processor);
        processor.connect(audioCtx.destination);

        // Send start conversation message
        console.log("Sending start_conversation message");
        ws.send(JSON.stringify({ type: "start_conversation" }));

        document.getElementById("voiceBtn").textContent = "â¹ï¸";
        document.getElementById("status").textContent = "KonuÅŸunâ€¦";

    } catch (error) {
        console.error("Error starting recording:", error);
        document.getElementById("status").textContent = "Mikrofon hatasÄ±: " + error.message;
    }
});

function downsample(buffer, inRate, outRate) {
    if (inRate === outRate) return buffer;
    if (outRate > inRate) {
        console.warn(`Output rate (${outRate}) higher than input rate (${inRate})`);
        return buffer;
    }

    const ratio = inRate / outRate;
    const newLength = Math.floor(buffer.length / ratio);
    const result = new Float32Array(newLength);

    if (newLength === 0) {
        console.warn("Downsampling resulted in zero-length buffer");
        return new Float32Array(0);
    }

    for (let i = 0; i < newLength; i++) {
        const startIdx = Math.floor(i * ratio);
        const endIdx = Math.min(Math.floor((i + 1) * ratio), buffer.length);
        let sum = 0;
        let count = 0;

        for (let j = startIdx; j < endIdx; j++) {
            sum += buffer[j];
            count++;
        }

        result[i] = count > 0 ? sum / count : 0;
    }
    return result;
}

function floatToPCM16(floatBuf) {
    if (!floatBuf || floatBuf.length === 0) {
        console.warn("Empty float buffer provided to floatToPCM16");
        return new Int16Array(0);
    }

    const int16 = new Int16Array(floatBuf.length);
    for (let i = 0; i < floatBuf.length; i++) {
        const s = Math.max(-1, Math.min(1, floatBuf[i])); // clamp
        int16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16;
}

let playAudioCtx, playbackTime = 0;
const activeSources = [];

function playAudio(arrayBuf) {
    console.log(`playAudio called with ${arrayBuf.byteLength} bytes`);

    if (!playAudioCtx) {
        playAudioCtx = new (window.AudioContext || window.webkitAudioContext)();
        console.log(`Created playback AudioContext with sample rate: ${playAudioCtx.sampleRate}`);
    }

    const int16 = new Int16Array(arrayBuf);
    const float32 = new Float32Array(int16.length);

    for (let i = 0; i < int16.length; i++) {
        float32[i] = int16[i] / (int16[i] < 0 ? 0x8000 : 0x7FFF);
    }

    console.log(`Converting ${int16.length} samples to audio buffer`);
    const audioBuffer = playAudioCtx.createBuffer(1, float32.length, 24000);
    audioBuffer.copyToChannel(float32, 0);

    const src = playAudioCtx.createBufferSource();
    src.buffer = audioBuffer;
    src.connect(playAudioCtx.destination);

    const now = playAudioCtx.currentTime;
    if (playbackTime < now) playbackTime = now;
    src.start(playbackTime);
    playbackTime += audioBuffer.duration;

    src.onended = () => {
        const idx = activeSources.indexOf(src);
        if (idx > -1) activeSources.splice(idx, 1);
        if (playAudioCtx.currentTime + 0.5 > playbackTime)
            playbackTime = playAudioCtx.currentTime;
    };
    activeSources.push(src);
}


let isMuted = false;
const muteBtn = document.getElementById("muteBtn");

// Mute button functionality
muteBtn.addEventListener("click", () => {
    isMuted = !isMuted;
    muteBtn.textContent = isMuted ? "ðŸ”ˆ" : "ðŸ”‡";
    muteBtn.title = isMuted ? "Unmute microphone" : "Mute microphone";
    document.getElementById("status").textContent = isMuted
        ? "Muted (listening)"
        : "Speak";

    console.warn("Ws is not open, cannot send mute_state");
});


class AudioVisualizer {
    constructor(canvasId) {
        this.canvas = document.getElementById(canvasId);
        this.ctx = this.canvas.getContext("2d");
        this.audioCtx = null;
        this.analyser = null;
        this.rafId = null;
        this.canvas.width = this.canvas.offsetWidth;
        this.canvas.height = this.canvas.offsetHeight;
    }
    start(stream) {
        if (!this.audioCtx)
            this.audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        this.analyser = this.audioCtx.createAnalyser();
        this.analyser.fftSize = 256;
        this.analyser.smoothingTimeConstant = 0.7;
        this.audioCtx.createMediaStreamSource(stream).connect(this.analyser);
        this.draw();
    }
    draw() {
        if (!this.analyser) return;
        const bufLen = this.analyser.frequencyBinCount;
        const data = new Uint8Array(bufLen);
        this.analyser.getByteFrequencyData(data);
        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        const barW = (this.canvas.width / bufLen) * 2.5;
        let x = 0;
        for (let i = 0; i < bufLen; i++) {
            const h = (data[i] / 255) * this.canvas.height * 0.6;
            const g = this.ctx.createLinearGradient(0, 0, 0, this.canvas.height);
            g.addColorStop(0, "#4facfe");
            g.addColorStop(1, "#00f2fe");
            this.ctx.fillStyle = g;
            this.ctx.fillRect(x, this.canvas.height - h, barW, h);
            x += barW + 1;
        }
        this.rafId = requestAnimationFrame(() => this.draw());
        if (isMuted) {
            this.ctx.fillStyle = "rgba(255, 50, 50, 0.5)";
            this.ctx.fillRect(0, 0, this.canvas.width, this.canvas.height);

            this.ctx.fillStyle = "white";
            this.ctx.font = "20px Arial";
            this.ctx.textAlign = "center";
            this.ctx.fillText("MUTED", this.canvas.width / 2, this.canvas.height / 2);
        }
    }
    stop() {
        cancelAnimationFrame(this.rafId);
        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
    }
}

const viz = new AudioVisualizer("audioVisualizer");